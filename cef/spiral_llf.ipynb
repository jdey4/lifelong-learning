{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier    \n",
    "        \n",
    "# Should use this class to generate CIFAR fig..\n",
    "class LifelongForest:\n",
    "    \"\"\"\n",
    "    Lifelong Forest class.\n",
    "    \"\"\"\n",
    "    def __init__(self, acorn=None):\n",
    "        \"\"\"\n",
    "        Two major things the Forest Class needs access to:\n",
    "            1) the realized random forest model (self.models_ is a list of forests, 1 for each task)\n",
    "            2) old data (to update posteriors when a new task is introduced)\n",
    "        \"\"\"\n",
    "        self.models_ = []\n",
    "        self.X_ = []\n",
    "        self.y_ = []\n",
    "        self.n_tasks = 0\n",
    "        self.labels = set()\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "    \n",
    "    def new_forest(self, X, y, n_estimators=200, max_samples=0.32,\n",
    "                        bootstrap=True, max_depth=30, min_samples_leaf=1,\n",
    "                        acorn=None):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        X: an array-like object of features; X.shape == (n_samples, n_features)\n",
    "        y: an array-like object of class labels; len(y) == n_samples\n",
    "        n_estimators: int; number of trees to construct (default = 200)\n",
    "        max_samples: float in (0, 1]: number of samples to consider when \n",
    "            constructing a new tree (default = 0.32)\n",
    "        bootstrap: bool; If True then the samples are sampled with replacement\n",
    "        max_depth: int; maximum depth of a tree\n",
    "        min_samples_leaf: int; minimum number of samples in a leaf node\n",
    "        \n",
    "        Return\n",
    "        model: a BaggingClassifier fit to X, y\n",
    "        \"\"\"\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            raise ValueError('1d data will cause headaches down the road')\n",
    "            \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "            \n",
    "        self.X_.append(X)\n",
    "        self.y_.append(y)\n",
    "            \n",
    "        \n",
    "        max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "\n",
    "        model=BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                         max_features = max_features),\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  max_samples=max_samples,\n",
    "                                  bootstrap=bootstrap)\n",
    "\n",
    "        model.fit(X, y)\n",
    "        self.models_.append(model)\n",
    "        self.n_tasks += 1\n",
    "        #changed to accommodate different number of classes for each predictor\n",
    "        self.labels |=set(y)\n",
    "        \n",
    "        return model\n",
    "    \n",
    " \n",
    "    \n",
    "    def _get_leaves(self, estimator):\n",
    "        \"\"\"\n",
    "        Internal function to get leaf node ids of estimator.\n",
    "        \n",
    "        Input\n",
    "        estimator: a fit DecisionTreeClassifier\n",
    "        \n",
    "        Return\n",
    "        leaf_ids: numpy array; an array of leaf node ids\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        # adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "        n_nodes = estimator.tree_.node_count\n",
    "        children_left = estimator.tree_.children_left\n",
    "        children_right = estimator.tree_.children_right\n",
    "        feature = estimator.tree_.feature\n",
    "        threshold = estimator.tree_.threshold\n",
    "\n",
    "        leaf_ids = []\n",
    "        stack = [(0, -1)] \n",
    "        while len(stack) > 0:\n",
    "            node_id, parent_depth = stack.pop()\n",
    "\n",
    "            # If we have a test node\n",
    "            if (children_left[node_id] != children_right[node_id]):\n",
    "                stack.append((children_left[node_id], parent_depth + 1))\n",
    "                stack.append((children_right[node_id], parent_depth + 1))\n",
    "            else:\n",
    "                leaf_ids.append(node_id)\n",
    "\n",
    "        return np.array(leaf_ids)\n",
    "    \n",
    "    \n",
    "    def _finite_sample_correction(self, class_probs, row_sums):\n",
    "        \"\"\"\n",
    "        An internal function for finite sample correction of posterior estimation.\n",
    "        \n",
    "        Input\n",
    "        class_probs: numpy array; array of posteriors to correct\n",
    "        row_sums: numpy array; array of partition counts\n",
    "        \n",
    "        Output\n",
    "        class_probs: numpy array; finite sample corrected posteriors\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "        \n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "    \n",
    "        return class_probs\n",
    "    \n",
    "    \n",
    "    def _estimate_posteriors(self, test, representation=0, decider=0, subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        An internal function to estimate the posteriors.\n",
    "        \n",
    "        Input\n",
    "        task_number: int; indicates which model in self.model_ to use\n",
    "        test: array-like; test observation\n",
    "        in_task: bool; True if test is an in-task observation(s)\n",
    "        subsample: float in (0, 1]; proportion of out-of-task samples to use to\n",
    "            estimate posteriors\n",
    "            \n",
    "        Return\n",
    "        probs: numpy array; probs[i, k] is the probability of observation i\n",
    "            being class k\n",
    "            \n",
    "        Usage\n",
    "        predict(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        if acorn is not None:\n",
    "            acorn = np.random.seed(acorn)\n",
    "            \n",
    "        if representation==decider:\n",
    "            in_task=True\n",
    "        else:\n",
    "            in_task=False\n",
    "            \n",
    "        train = self.X_[decider]\n",
    "        y = self.y_[decider]\n",
    "            \n",
    "        model = self.models_[representation]\n",
    "\n",
    "        n, d = train.shape\n",
    "        \n",
    "        if test.ndim > 1:\n",
    "            m, d_ = test.shape\n",
    "        else:\n",
    "            m = len(test)\n",
    "            d_ = 1\n",
    "\n",
    "        #changed to accommodate different number of classes for each predictor\n",
    "        size = len(self.labels)\n",
    "        class_counts = np.zeros((m, size))\n",
    "        \n",
    "\n",
    "        for tree in model:\n",
    "            # get out of bag indicies\n",
    "            if in_task:\n",
    "                prob_indices = _generate_unsampled_indices(tree.random_state, n)              \n",
    "                # in_bag_idx = _generate_sample_indices(tree.random_state, n) # this is not behaving as i expected\n",
    "            else:\n",
    "                prob_indices = np.random.choice(range(n), size=int(subsample*n), replace=False)\n",
    "\n",
    "            leaf_nodes = self._get_leaves(tree)\n",
    "            unique_leaf_nodes = np.unique(leaf_nodes)\n",
    "        \n",
    "            # get all node counts\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            \n",
    "            #changed to accomendate different number of classes for each predictor\n",
    "            #get max of n_clasees and y labels\n",
    "            posterior_class_counts = np.zeros((len(unique_leaf_nodes), size))\n",
    "            \n",
    "            for prob_index in prob_indices:\n",
    "\n",
    "                temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "                tt = np.where(unique_leaf_nodes == temp_node)\n",
    "\n",
    "                posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "       \n",
    "            # total number of points in a node\n",
    "            row_sums = posterior_class_counts.sum(axis=1)\n",
    "\n",
    "            # no divide by zero\n",
    "            row_sums[row_sums == 0] = 1\n",
    "\n",
    "            # posteriors\n",
    "            class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "            # posteriors with finite sampling correction\n",
    "\n",
    "            class_probs = self._finite_sample_correction(class_probs, row_sums)\n",
    "\n",
    "            # posteriors as a list\n",
    "            class_probs.tolist()\n",
    "\n",
    "            \n",
    "            # get probability for out of bag samples\n",
    "   \n",
    "            partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "            eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "            \n",
    "            eval_class_probs = np.array(eval_class_probs)\n",
    "            # find total elements for out of bag samples\n",
    "            elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "            # store counts for each x (repeat fhis for each tree)\n",
    "            class_counts += elems\n",
    "        \n",
    "        \n",
    "        # calculate p(y|X = x) for all x's\n",
    "        probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def predict(self, test, representation=0, decider='all', subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each sample in test.\n",
    "        \n",
    "        Input\n",
    "        test: array-like; either a 1d array of length n_features\n",
    "            or a 2d array of shape (m, n_features) \n",
    "        task_number: int; task number \n",
    "        \"\"\"\n",
    "        sum_posteriors =  np.zeros((test.shape[0], len(self.labels)))\n",
    "        \n",
    "        \n",
    "        if representation is 'all':\n",
    "            for i in range(self.n_tasks):\n",
    "                sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                            i,\n",
    "                                                            decider,\n",
    "                                                            subsample,\n",
    "                                                            acorn)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                        representation,\n",
    "                                                        decider,\n",
    "                                                        subsample,\n",
    "                                                        acorn)\n",
    "        return np.argmax(sum_posteriors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSpirals(N, D=2, K=5, noise = 0.5, acorn = None, density=0.3):\n",
    "\n",
    "    #N number of poinst per class\n",
    "    #D number of features, \n",
    "    #K number of classes\n",
    "    X = []\n",
    "    Y = []\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    if K == 2:\n",
    "        turns = 2\n",
    "    elif K==3:\n",
    "        turns = 2.5\n",
    "    elif K==5:\n",
    "        turns = 3.5\n",
    "    else:\n",
    "        print (\"sorry, can't currently surpport %s classes \" %K)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if K == 2:\n",
    "        r = np.linspace(0.01, 1, N)\n",
    "        t = np.linspace(0,  np.pi* 4 * turns/K, N) + noise * np.random.normal(0, density, N)\n",
    "        dx = r * np.cos(t)\n",
    "        dy = r* np.sin(t)\n",
    "\n",
    "        X.append(np.vstack([dx, dy]).T )\n",
    "        X.append(np.vstack([-dx, -dy]).T)\n",
    "        Y += [0] * N \n",
    "        Y += [1] * N\n",
    "    else:    \n",
    "        for j in range(1, K+1):\n",
    "            r = np.linspace(0.01, 1, N)\n",
    "            t = np.linspace((j-1) * np.pi *4 *turns/K,  j* np.pi * 4* turns/K, N) + noise * np.random.normal(0, density, N)\n",
    "            dx = r * np.cos(t)\n",
    "            dy = r* np.sin(t)\n",
    "\n",
    "            dd = np.vstack([dx, dy]).T        \n",
    "            X.append(dd)\n",
    "            #label\n",
    "            Y += [j-1] * N\n",
    "    return np.vstack(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spirals_experiment(N, D=2, K=5, noise = 0.5, acorn = None, density=0.3):\n",
    "\n",
    "    #N number of poinst per class\n",
    "    #D number of features, \n",
    "    #K number of classes\n",
    "    X = []\n",
    "    Y = []\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    if K == 2:\n",
    "        turns = 2\n",
    "    elif K==3:\n",
    "        turns = 2.5\n",
    "    elif K==5:\n",
    "        turns = 3.5\n",
    "    else:\n",
    "        print (\"sorry, can't currently surpport %s classes \" %K)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if K == 2:\n",
    "#         r = np.linspace(0.01, 1, N)\n",
    "        r = np.random.uniform(0,1,size=N)\n",
    "        t = np.linspace(0,  np.pi* 4 * turns/K, N) + noise * np.random.normal(0, density, N)\n",
    "        dx = r * np.cos(t)\n",
    "        dy = r* np.sin(t)\n",
    "\n",
    "        X.append(np.vstack([dx, dy]).T )\n",
    "        X.append(np.vstack([-dx, -dy]).T)\n",
    "        Y += [0] * N \n",
    "        Y += [1] * N\n",
    "    else:    \n",
    "        for j in range(1, K+1):\n",
    "            r = np.linspace(0.01, 1, N)\n",
    "            t = np.linspace((j-1) * np.pi *4 *turns/K,  j* np.pi * 4* turns/K, N) + noise * np.random.normal(0, density, N)\n",
    "            dx = r * np.cos(t)\n",
    "            dy = r* np.sin(t)\n",
    "\n",
    "            dd = np.vstack([dx, dy]).T        \n",
    "            X.append(dd)\n",
    "            #label\n",
    "            Y += [j-1] * N\n",
    "    return np.vstack(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiral_transfer(func, nx, nz, test_size,x_class=2, z_class=3,tree_size=None):\n",
    "    \n",
    "    X, labelsX = func(nx, K=x_class, acorn=1234, noise=0.8, density=0.1)\n",
    "    testX, test_labelsX = func(test_size, K=x_class, acorn=4567, noise=0.8, density=0.1)\n",
    "        \n",
    "    Z, labelsZ = func(nz, K=z_class, acorn=1234, noise=0.8, density=0.1)\n",
    "    testZ, test_labelsZ = func(test_size, K=z_class, acorn=4567, noise=0.8, density=0.1)\n",
    "    \n",
    "\n",
    "    \n",
    "    if tree_size is None:\n",
    "        Tx = int(np.floor(np.sqrt(X.shape[0])))\n",
    "    else:\n",
    "        Tx = tree_size\n",
    "    \n",
    "    Tz = int(np.floor(np.sqrt(Z.shape[0])))\n",
    "    \n",
    "\n",
    "    lifelong_forest = LifelongForest()\n",
    "        \n",
    "        \n",
    "    lifelong_forest.new_forest(X, labelsX, n_estimators=Tx)\n",
    "    lifelong_forest.new_forest(Z, labelsZ, n_estimators=Tz)\n",
    "    \n",
    "    llf_task1=lifelong_forest.predict(testX, representation='all', decider=0)\n",
    "    llf_task2=lifelong_forest.predict(testZ, representation='all', decider=1)\n",
    "\n",
    "    tf_task1=lifelong_forest.predict(testX, representation=1, decider=0)\n",
    "    tf_task2=lifelong_forest.predict(testZ, representation=0, decider=1)\n",
    "\n",
    "    df_task1=lifelong_forest.predict(testX, representation=0, decider=0)\n",
    "    df_task2=lifelong_forest.predict(testZ, representation=1, decider=1)\n",
    "\n",
    "\n",
    "    #llf error\n",
    "    comb1 = 1 - np.sum(llf_task1 == test_labelsX)/len(test_labelsX)\n",
    "    comb2 = 1 - np.sum(llf_task2 == test_labelsZ)/len(test_labelsZ)\n",
    "    \n",
    "    #tr error\n",
    "    cross1 = 1 - np.sum(tf_task1 == test_labelsX)/len(test_labelsX)\n",
    "    cross2 = 1 - np.sum(tf_task2 == test_labelsZ)/len(test_labelsZ)\n",
    "    \n",
    "    \n",
    "    #rf error\n",
    "    base1 = 1 - np.sum(df_task1 == test_labelsX)/len(test_labelsX)\n",
    "    base2 = 1 - np.sum(df_task2 == test_labelsZ)/len(test_labelsZ)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return base1, base2, cross1, cross2, comb1, comb2, \\\n",
    "            testX, test_labelsX, testZ,test_labelsZ, \\\n",
    "            df_task1, df_task2, tf_task1, tf_task2, llf_task1, llf_task2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotError(errArr, axis =None, xAxisLabel=None, x_marker=None):\n",
    "    errs = np.array(errArr)\n",
    "    if axis is  None:\n",
    "        axis = np.arange(10, (errs.shape[0]+1)*10, 10)\n",
    "    if xAxisLabel is None:\n",
    "        xAxisLabel = \"X train size\"\n",
    "\n",
    "    fig = plt.figure(figsize=(11,12))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    plt.title(\"X\")\n",
    "    ax1.plot(axis, errs[:,0], label='rf_x',linewidth=2 )\n",
    "    ax1.plot(axis, errs[:,2], label='tf_x',linewidth=2 )\n",
    "    ax1.plot(axis, errs[:,4], label='llf_x',linewidth=2 )\n",
    "    if x_marker is not None:\n",
    "        plt.axvline(x=x_marker, color='black')\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    plt.ylabel('Test error')\n",
    "    lgd = ax1.legend(handles, labels, loc='upper right', )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax2 = fig.add_subplot(212)\n",
    "    plt.title('Z')\n",
    "\n",
    "    ax2.plot(axis, errs[:,1], label='rf_z',linewidth=2 )\n",
    "    ax2.plot(axis, errs[:,3], label='tf_z', linewidth=2 )\n",
    "    ax2.plot(axis, errs[:,5], label='llf_z', linewidth=2 )\n",
    "    plt.ylabel('Test error')\n",
    "    plt.xlabel(xAxisLabel)\n",
    "    if x_marker is not None:\n",
    "        plt.axvline(x=x_marker, color='black')\n",
    "\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    lgd = ax2.legend(handles, labels, loc='upper right', )\n",
    "\n",
    "def plotRes(x, z, lx,lz, p0x, p0z, p1x, p1z, p2x, p2z, e0x, e0z, e1x, e1z, e2x, e2z):\n",
    "\n",
    "    colors = sns.color_palette('Dark2')\n",
    "    cx, cx0, cx1, cx2 = [], [], [], []\n",
    "    cz, cz0, cz1, cz2 = [], [], [], []\n",
    "    \n",
    "    for xx, x0, x1, x2 in zip(lx, p0x, p1x, p2x):\n",
    "        cx. append(colors[xx])\n",
    "        cx0.append(colors[x0])\n",
    "        cx1.append(colors[x1])\n",
    "        cx2.append(colors[x2])\n",
    "        \n",
    "    for zz, z0, z1, z2 in zip(lz, p0z, p1z, p2z):\n",
    "        cz.append(colors[zz])\n",
    "        cz0.append(colors[z0])\n",
    "        cz1.append(colors[z1])\n",
    "        cz2.append(colors[z2])\n",
    "        \n",
    "    sns.set()\n",
    "    fig0 = plt.figure(figsize=(8, 16))\n",
    "    fig0.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "\n",
    "    ax01 = fig0.add_subplot(4, 2, 1, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax02 = fig0.add_subplot(4, 2, 2, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax03 = fig0.add_subplot(4, 2, 3, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax04 = fig0.add_subplot(4, 2, 4, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax05 = fig0.add_subplot(4, 2, 5, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax06 = fig0.add_subplot(4, 2, 6, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax07 = fig0.add_subplot(4, 2, 7, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    ax08 = fig0.add_subplot(4, 2, 8, xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "    \n",
    "\n",
    "    ax01.scatter(x[:, 0], x[:, 1], c = cx)\n",
    "    ax02.scatter(z[:, 0], z[:, 1], c = cz)\n",
    "\n",
    "    ax03.scatter(x[:, 0], x[:, 1], c = cx0)\n",
    "    ax04.scatter(z[:, 0], z[:, 1], c = cz0)\n",
    "    \n",
    "    ax05.scatter(x[:, 0], x[:, 1], c = cx1)\n",
    "    ax06.scatter(z[:, 0], z[:, 1], c = cz1)\n",
    "    \n",
    "    ax07.scatter(x[:, 0], x[:, 1], c = cx2)\n",
    "    ax08.scatter(z[:, 0], z[:, 1], c = cz2)\n",
    "\n",
    "\n",
    "    ax01.set_title('X_test', fontsize=20)\n",
    "    ax02.set_title('Z_test', fontsize=20)\n",
    "\n",
    "    ax03.set_title('X_rf error=%.3f' %e0x, fontsize=20)\n",
    "    ax04.set_title('Z_rf error=%.3f' %e0z,  fontsize=20)\n",
    "\n",
    "    \n",
    "    ax05.set_title('X_tf error=%.3f' %e1x, fontsize=20)\n",
    "    ax06.set_title('Z_tf error=%.3f' %e1z,  fontsize=20)\n",
    "    \n",
    "    ax07.set_title('X_llf error=%.3f' %e2x, fontsize=20)\n",
    "    ax08.set_title('Z_llf error=%.3f' %e2z,  fontsize=20)\n",
    "    \n",
    "\n",
    "    fig0.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "errs = []\n",
    "axis = []\n",
    "func = spirals_experiment\n",
    "for i in range(10, 15):\n",
    "    nx =10 *(i+1)\n",
    "    nz = 50 # * (i+1)\n",
    "    tree_size = 70\n",
    "    axis.append(nx)\n",
    "    e_rf0, e_rf1, e_tf0, e_tf1, e_llf0, e_llf1,\\\n",
    "    x, lx, z, lz, \\\n",
    "    l_rf0, l_rf1, l_tf0, l_tf1, l_llf0, l_llf1 = spiral_transfer(func, nx, nz, test_size=1000, \n",
    "                                                                     x_class=2, z_class=3,\n",
    "                                                                    tree_size=tree_size)\n",
    "\n",
    "    print(\"xSize:%s, zSize:%s n_estimator: %s\" %(nx, nz, tree_size))\n",
    "    print(\"forest results: rf_x %0.2f, rf_z: %.2f, tf_x: %.2f, tf_z: %.2f, llf_x:%.2f, llf_z:%.2f\" \n",
    "          %(e_rf0, e_rf1, e_tf0, e_tf1, e_llf0, e_llf1))\n",
    "    errs.append((e_rf0, e_rf1, e_tf0, e_tf1, e_llf0, e_llf1))\n",
    "    \n",
    "    if e_rf0< 0.005 or e_llf1 <0.01:\n",
    "        print ('done')\n",
    "        break\n",
    "\n",
    "plotRes(x, z, lx,lz, l_rf0, l_rf1, l_tf0, l_tf1, l_llf0, l_llf1, e_rf0, e_rf1, e_tf0, e_tf1, e_llf0, e_llf1)\n",
    "plotError(errs, axis=axis,xAxisLabel='x training size', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh",
   "language": "python",
   "name": "hh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
