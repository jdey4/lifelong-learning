{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelong_forests import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_tasks = 10 # should divide 100 evenly\n",
    "# K = int(len(class_idx)/n_tasks)\n",
    "\n",
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def homogenize_labels(a):\n",
    "    u = np.unique(a)\n",
    "    return np.array([np.where(u == i)[0][0] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifelongForest:\n",
    "    \"\"\"\n",
    "    Lifelong Forest class.\n",
    "    \"\"\"\n",
    "    def __init__(self, acorn=None):\n",
    "        \"\"\"\n",
    "        Two major things the Forest Class needs access to:\n",
    "            1) the realized random forest model (self.models_ is a list of forests, 1 for each task)\n",
    "            2) old data (to update posteriors when a new task is introduced)\n",
    "        \"\"\"\n",
    "        self.models_ = []\n",
    "        self.X_ = []\n",
    "        self.y_ = []\n",
    "        self.n_tasks = 0\n",
    "        self.n_classes = None\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "    \n",
    "    def new_forest(self, X, y, n_estimators=200, max_samples=0.32,\n",
    "                        bootstrap=True, max_depth=30, min_samples_leaf=1,\n",
    "                        acorn=None):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        X: an array-like object of features; X.shape == (n_samples, n_features)\n",
    "        y: an array-like object of class labels; len(y) == n_samples\n",
    "        n_estimators: int; number of trees to construct (default = 200)\n",
    "        max_samples: float in (0, 1]: number of samples to consider when \n",
    "            constructing a new tree (default = 0.32)\n",
    "        bootstrap: bool; If True then the samples are sampled with replacement\n",
    "        max_depth: int; maximum depth of a tree\n",
    "        min_samples_leaf: int; minimum number of samples in a leaf node\n",
    "        \n",
    "        Return\n",
    "        model: a BaggingClassifier fit to X, y\n",
    "        \"\"\"\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            raise ValueError('1d data will cause headaches down the road')\n",
    "            \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "            \n",
    "        self.X_.append(X)\n",
    "        self.y_.append(y.astype(int))\n",
    "            \n",
    "        n = X.shape[0]\n",
    "        K = len(np.unique(y))\n",
    "        \n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = K\n",
    "        \n",
    "        max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "\n",
    "        model=BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                         max_features = max_features),\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  max_samples=max_samples,\n",
    "                                  bootstrap=bootstrap)\n",
    "\n",
    "        model.fit(X, y)\n",
    "        self.models_.append(model)\n",
    "        self.n_tasks += 1\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def _get_leaves(self, estimator):\n",
    "        \"\"\"\n",
    "        Internal function to get leaf node ids of estimator.\n",
    "        \n",
    "        Input\n",
    "        estimator: a fit DecisionTreeClassifier\n",
    "        \n",
    "        Return\n",
    "        leaf_ids: numpy array; an array of leaf node ids\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        # adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "        n_nodes = estimator.tree_.node_count\n",
    "        children_left = estimator.tree_.children_left\n",
    "        children_right = estimator.tree_.children_right\n",
    "        feature = estimator.tree_.feature\n",
    "        threshold = estimator.tree_.threshold\n",
    "\n",
    "        leaf_ids = []\n",
    "        stack = [(0, -1)] \n",
    "        while len(stack) > 0:\n",
    "            node_id, parent_depth = stack.pop()\n",
    "\n",
    "            # If we have a test node\n",
    "            if (children_left[node_id] != children_right[node_id]):\n",
    "                stack.append((children_left[node_id], parent_depth + 1))\n",
    "                stack.append((children_right[node_id], parent_depth + 1))\n",
    "            else:\n",
    "                leaf_ids.append(node_id)\n",
    "\n",
    "        return np.array(leaf_ids)\n",
    "    \n",
    "    \n",
    "    def _finite_sample_correction(self, class_probs, row_sums):\n",
    "        \"\"\"\n",
    "        An internal function for finite sample correction of posterior estimation.\n",
    "        \n",
    "        Input\n",
    "        class_probs: numpy array; array of posteriors to correct\n",
    "        row_sums: numpy array; array of partition counts\n",
    "        \n",
    "        Output\n",
    "        class_probs: numpy array; finite sample corrected posteriors\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "    \n",
    "        return class_probs\n",
    "    \n",
    "    \n",
    "    def _estimate_posteriors(self, test, representation=0, decider=0, subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        An internal function to estimate the posteriors.\n",
    "        \n",
    "        Input\n",
    "        task_number: int; indicates which model in self.model_ to use\n",
    "        test: array-like; test observation\n",
    "        in_task: bool; True if test is an in-task observation(s)\n",
    "        subsample: float in (0, 1]; proportion of out-of-task samples to use to\n",
    "            estimate posteriors\n",
    "            \n",
    "        Return\n",
    "        probs: numpy array; probs[i, k] is the probability of observation i\n",
    "            being class k\n",
    "            \n",
    "        Usage\n",
    "        predict(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        if acorn is not None:\n",
    "            acorn = np.random.seed(acorn)\n",
    "            \n",
    "        if representation==decider:\n",
    "            in_task=True\n",
    "        else:\n",
    "            in_task=False\n",
    "            \n",
    "        train = self.X_[decider]\n",
    "        y = self.y_[decider]\n",
    "            \n",
    "        model = self.models_[representation]\n",
    "\n",
    "        n, d = train.shape\n",
    "        \n",
    "        if test.ndim > 1:\n",
    "            m, d_ = test.shape\n",
    "        else:\n",
    "            m = len(test)\n",
    "            d_ = 1\n",
    "\n",
    "        class_counts = np.zeros((m, model.n_classes_))\n",
    "        for idx, tree in enumerate(model):\n",
    "            # get out of bag indicies\n",
    "            \n",
    "           \n",
    "            if in_task:\n",
    "                sampled_indices = model.estimators_samples_[idx]\n",
    "                prob_indices = np.delete(range(n), sampled_indices)\n",
    "            else:\n",
    "                prob_indices = np.random.choice(range(n), size=int(subsample*n), replace=False)\n",
    "\n",
    "            leaf_nodes = self._get_leaves(tree)\n",
    "            unique_leaf_nodes = np.unique(leaf_nodes)\n",
    "\n",
    "            # get all node counts\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            # get probs for eval samples\n",
    "            posterior_class_counts = np.zeros((len(unique_leaf_nodes), model.n_classes_))\n",
    "\n",
    "            for prob_index in prob_indices:\n",
    "                temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "                posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "\n",
    "            # total number of points in a node\n",
    "            row_sums = posterior_class_counts.sum(axis=1)\n",
    "\n",
    "            # no divide by zero\n",
    "            row_sums[row_sums == 0] = 1\n",
    "\n",
    "            # posteriors\n",
    "            class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "            # posteriors with finite sampling correction\n",
    "\n",
    "            class_probs = self._finite_sample_correction(class_probs, row_sums)\n",
    "\n",
    "            # posteriors as a list\n",
    "            class_probs.tolist()\n",
    "\n",
    "            partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "            # get probability for out of bag samples\n",
    "            eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "            eval_class_probs = np.array(eval_class_probs)\n",
    "            # find total elements for out of bag samples\n",
    "            elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "            # store counts for each x (repeat fhis for each tree)\n",
    "            class_counts += elems\n",
    "        # calculate p(y|X = x) for all x's\n",
    "        probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def predict(self, test, representation=0, decider='all', subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each sample in test.\n",
    "        \n",
    "        Input\n",
    "        test: array-like; either a 1d array of length n_features\n",
    "            or a 2d array of shape (m, n_features) \n",
    "        task_number: int; task number \n",
    "        \"\"\"\n",
    "        \n",
    "        sum_posteriors = np.zeros((test.shape[0], self.n_classes))\n",
    "        \n",
    "        if representation is 'all':\n",
    "            representation = np.arange(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        elif isinstance(representation, list):\n",
    "            representation = np.array(representation)\n",
    "            \n",
    "        if not isinstance(representation, np.ndarray):\n",
    "            raise ValueError('bad representation type %s: int, list of ints or numpy arrays only'%(str(type(representation)))\n",
    "                            )\n",
    "        else:\n",
    "            representation = representation.astype(int)\n",
    "            \n",
    "        for i, rep in enumerate(representation):\n",
    "            sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                        i,\n",
    "                                                        decider,\n",
    "                                                        subsample,\n",
    "                                                        acorn)            \n",
    "                \n",
    "        return np.argmax(sum_posteriors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'cifar-100-python/train'\n",
    "unpickled_train = unpickle(train_file)\n",
    "train_keys = list(unpickled_train.keys())\n",
    "fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "\n",
    "train_data = unpickled_train[list(train_keys)[-1]]\n",
    "class_idx = [np.where(fine_labels == u)[0] for u in np.unique(fine_labels)]\n",
    "\n",
    "train_by_task = [np.concatenate(class_idx[i*n_tasks: (i+1)*n_tasks]) for i in range(n_tasks)]\n",
    "\n",
    "K = int(len(class_idx)/n_tasks)\n",
    "\n",
    "test_file = 'cifar-100-python/test'\n",
    "unpickled_test = unpickle(test_file)\n",
    "test_keys = list(unpickled_test.keys())\n",
    "test_labels = np.array(unpickled_test[test_keys[2]])\n",
    "\n",
    "test_data = unpickled_test[test_keys[-1]]\n",
    "test_class_idx = [np.where(test_labels == u)[0] for u in np.unique(test_labels)]\n",
    "test_by_task = [np.concatenate(test_class_idx[i*n_tasks: (i+1)*n_tasks]) for i in range(n_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "lifelong_forest = LifelongForest()\n",
    "\n",
    "n_trees=10\n",
    "\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    X = train_data[np.concatenate(class_idx[i*n_tasks: (i+1)*n_tasks])]\n",
    "    labels = homogenize_labels(np.concatenate([n_tasks*i*np.ones(500) + j for j in range(n_tasks*i, n_tasks*(i+1))]))\n",
    "    if i > 0:\n",
    "        np.random.shuffle(labels)\n",
    "    lifelong_forest.new_forest(X, labels, n_estimators=n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211.6, 606.3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_nodes = np.zeros(n_tasks)\n",
    "node_counts=np.zeros(n_tasks)\n",
    "for i, f in enumerate(lifelong_forest.models_):\n",
    "    node_counts[i]=f.estimators_[0].tree_.node_count\n",
    "    leaf_nodes[i]=f.estimators_[0].get_n_leaves()\n",
    "    \n",
    "np.mean(node_counts), np.mean(leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_errors = np.zeros(n_tasks)\n",
    "homogenized_labels = [homogenize_labels(test_labels[t]) for t in test_by_task[:n_tasks]]\n",
    "llf_errors = [np.zeros((n_tasks-i)) for i in range(n_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:11<01:45, 11.75s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:25<01:38, 12.26s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:46<01:45, 15.02s/it]\u001b[A\n",
      " 40%|████      | 4/10 [01:17<01:58, 19.80s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:56<02:08, 25.65s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [02:44<02:09, 32.31s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [03:37<01:55, 38.40s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [04:40<01:31, 45.76s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [05:51<00:53, 53.35s/it]\u001b[A\n",
      " 10%|█         | 1/10 [07:11<1:04:43, 431.49s/it]A\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 1/9 [00:22<02:57, 22.21s/it]\u001b[A\n",
      " 22%|██▏       | 2/9 [00:44<02:35, 22.24s/it]\u001b[A\n",
      " 33%|███▎      | 3/9 [01:15<02:28, 24.72s/it]\u001b[A\n",
      " 44%|████▍     | 4/9 [01:52<02:23, 28.63s/it]\u001b[A\n",
      " 56%|█████▌    | 5/9 [02:40<02:16, 34.23s/it]\u001b[A\n",
      " 67%|██████▋   | 6/9 [03:35<02:01, 40.56s/it]\u001b[A\n",
      " 78%|███████▊  | 7/9 [04:38<01:34, 47.21s/it]\u001b[A\n",
      " 89%|████████▉ | 8/9 [05:49<00:54, 54.43s/it]\u001b[A\n",
      " 20%|██        | 2/10 [14:20<57:25, 430.68s/it]  \n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:30<03:34, 30.59s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [01:00<03:02, 30.36s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [01:39<02:45, 33.07s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [02:26<02:29, 37.30s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [03:22<02:07, 42.64s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [04:25<01:37, 48.88s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [05:49<00:59, 59.32s/it]\u001b[A\n",
      " 30%|███       | 3/10 [21:34<50:22, 431.78s/it]A\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:41<04:06, 41.00s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [01:22<03:26, 41.23s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [02:13<02:56, 44.20s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [03:12<02:25, 48.53s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [04:20<01:48, 54.31s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [05:37<01:01, 61.04s/it]\u001b[A\n",
      " 40%|████      | 4/10 [28:37<42:54, 429.01s/it]A\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:49<04:09, 49.84s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [01:41<03:21, 50.33s/it]\u001b[A\n",
      " 50%|█████     | 3/6 [02:39<02:38, 52.79s/it]\u001b[A\n",
      " 67%|██████▋   | 4/6 [03:46<01:54, 57.05s/it]\u001b[A\n",
      " 83%|████████▎ | 5/6 [05:02<01:02, 62.53s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [35:04<34:42, 416.58s/it]A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:57<03:50, 57.69s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:55<02:53, 57.67s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:01<02:00, 60.28s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [37:15<10:58, 658.31s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [1:12:51<1:04:46, 971.60s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:21<01:05, 21.85s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [00:45<00:44, 22.42s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [01:10<00:23, 23.23s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [1:14:29<35:29, 709.67s/it]  \n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:24<00:48, 24.34s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:50<00:24, 24.83s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [1:16:29<17:45, 532.53s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "subsample=1\n",
    "\n",
    "for i, test_set in enumerate(tqdm(test_by_task)):\n",
    "    if i != 0:\n",
    "        pass\n",
    "#         np.random.shuffle(homogenized_labels[i])\n",
    "    for j in tqdm(range(i, n_tasks)):\n",
    "        if i == j:\n",
    "            stl_temp_pred = lifelong_forest.predict(test_data[test_set],\n",
    "                                                representation=i,\n",
    "                                                decider=i,\n",
    "                                                subsample=subsample\n",
    "                                                )\n",
    "            stl_errors[i] = np.mean(stl_temp_pred == homogenized_labels[i])\n",
    "            \n",
    "            llf_temp_pred = lifelong_forest.predict(test_data[test_set],\n",
    "                                                   representation=np.arange(i+1),\n",
    "                                                   decider=i,\n",
    "                                                   subsample=subsample\n",
    "                                                   )\n",
    "            llf_errors[i][j-i] = np.mean(stl_temp_pred == homogenized_labels[i])\n",
    "        else:\n",
    "            llf_temp_pred = lifelong_forest.predict(test_data[test_set],\n",
    "                                                   representation=np.arange(j+1),\n",
    "                                                   decider=i,\n",
    "                                                   subsample=subsample\n",
    "                                                   )\n",
    "            llf_errors[i][j-i] = np.mean(llf_temp_pred == homogenized_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "c = sns.color_palette('Paired', n_colors=10)\n",
    "for i in range(n_tasks - 1):\n",
    "    ns = np.arange(i + 1, n_tasks + 1)\n",
    "    ax.plot(ns,(stl_errors[i]) /(np.array(llf_errors[i])) , label = 'task %i'%(i + 1), c=c[i])\n",
    "    \n",
    "ax.scatter(10, (stl_errors[-1]) / (llf_errors[-1]), c = c[9], label='task 10', s = 5)\n",
    "    \n",
    "ax.set_title('Lifelong Forests on CIFAR-10x10', fontsize=20)\n",
    "ax.set_xlabel('Number of tasks seen', fontsize=18)\n",
    "ax.set_ylabel('Transfer Efficiency', fontsize=18)\n",
    "# ax.set_ylim(0.05 - 0.01, 0.5 + 0.01)\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.legend(loc='upper left', fontsize=14)\n",
    "ax.set_yticks([1, 1.1, 1.2, 1.3])\n",
    "ax.set_xticks(np.arange(1,11))\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ten_trees.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh",
   "language": "python",
   "name": "hh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
