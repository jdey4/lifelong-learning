{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should use this class to generate CIFAR fig..\n",
    "class LifelongForest:\n",
    "    \"\"\"\n",
    "    Lifelong Forest class.\n",
    "    \"\"\"\n",
    "    def __init__(self, acorn=None):\n",
    "        \"\"\"\n",
    "        Two major things the Forest Class needs access to:\n",
    "            1) the realized random forest model (self.models_ is a list of forests, 1 for each task)\n",
    "            2) old data (to update posteriors when a new task is introduced)\n",
    "        \"\"\"\n",
    "        self.models_ = []\n",
    "        self.X_ = []\n",
    "        self.y_ = []\n",
    "        self.n_tasks = 0\n",
    "        self.n_classes = None\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "    \n",
    "    def new_forest(self, X, y, n_estimators=200, max_samples=0.32,\n",
    "                        bootstrap=True, max_depth=30, min_samples_leaf=1,\n",
    "                        acorn=None):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        X: an array-like object of features; X.shape == (n_samples, n_features)\n",
    "        y: an array-like object of class labels; len(y) == n_samples\n",
    "        n_estimators: int; number of trees to construct (default = 200)\n",
    "        max_samples: float in (0, 1]: number of samples to consider when \n",
    "            constructing a new tree (default = 0.32)\n",
    "        bootstrap: bool; If True then the samples are sampled with replacement\n",
    "        max_depth: int; maximum depth of a tree\n",
    "        min_samples_leaf: int; minimum number of samples in a leaf node\n",
    "        \n",
    "        Return\n",
    "        model: a BaggingClassifier fit to X, y\n",
    "        \"\"\"\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            raise ValueError('1d data will cause headaches down the road')\n",
    "            \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "            \n",
    "        self.X_.append(X)\n",
    "        self.y_.append(y)\n",
    "            \n",
    "        n = X.shape[0]\n",
    "        K = len(np.unique(y))\n",
    "        \n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = K\n",
    "        \n",
    "        max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "\n",
    "        model=BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                         max_features = max_features),\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  max_samples=max_samples,\n",
    "                                  bootstrap=bootstrap)\n",
    "\n",
    "        model.fit(X, y)\n",
    "        self.models_.append(model)\n",
    "        self.n_tasks += 1\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def _get_leaves(self, estimator):\n",
    "        \"\"\"\n",
    "        Internal function to get leaf node ids of estimator.\n",
    "        \n",
    "        Input\n",
    "        estimator: a fit DecisionTreeClassifier\n",
    "        \n",
    "        Return\n",
    "        leaf_ids: numpy array; an array of leaf node ids\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        # adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "        n_nodes = estimator.tree_.node_count\n",
    "        children_left = estimator.tree_.children_left\n",
    "        children_right = estimator.tree_.children_right\n",
    "        feature = estimator.tree_.feature\n",
    "        threshold = estimator.tree_.threshold\n",
    "\n",
    "        leaf_ids = []\n",
    "        stack = [(0, -1)] \n",
    "        while len(stack) > 0:\n",
    "            node_id, parent_depth = stack.pop()\n",
    "\n",
    "            # If we have a test node\n",
    "            if (children_left[node_id] != children_right[node_id]):\n",
    "                stack.append((children_left[node_id], parent_depth + 1))\n",
    "                stack.append((children_right[node_id], parent_depth + 1))\n",
    "            else:\n",
    "                leaf_ids.append(node_id)\n",
    "\n",
    "        return np.array(leaf_ids)\n",
    "    \n",
    "    \n",
    "    def _finite_sample_correction(self, class_probs, row_sums):\n",
    "        \"\"\"\n",
    "        An internal function for finite sample correction of posterior estimation.\n",
    "        \n",
    "        Input\n",
    "        class_probs: numpy array; array of posteriors to correct\n",
    "        row_sums: numpy array; array of partition counts\n",
    "        \n",
    "        Output\n",
    "        class_probs: numpy array; finite sample corrected posteriors\n",
    "        \n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "    \n",
    "        return class_probs\n",
    "    \n",
    "    \n",
    "    def _estimate_posteriors(self, test, representation=0, decider=0, subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        An internal function to estimate the posteriors.\n",
    "        \n",
    "        Input\n",
    "        task_number: int; indicates which model in self.model_ to use\n",
    "        test: array-like; test observation\n",
    "        in_task: bool; True if test is an in-task observation(s)\n",
    "        subsample: float in (0, 1]; proportion of out-of-task samples to use to\n",
    "            estimate posteriors\n",
    "            \n",
    "        Return\n",
    "        probs: numpy array; probs[i, k] is the probability of observation i\n",
    "            being class k\n",
    "            \n",
    "        Usage\n",
    "        predict(..)\n",
    "        \"\"\"\n",
    "        \n",
    "        if acorn is not None:\n",
    "            acorn = np.random.seed(acorn)\n",
    "            \n",
    "        if representation==decider:\n",
    "            in_task=True\n",
    "        else:\n",
    "            in_task=False\n",
    "            \n",
    "        train = self.X_[decider]\n",
    "        y = self.y_[decider]\n",
    "            \n",
    "        model = self.models_[representation]\n",
    "\n",
    "        n, d = train.shape\n",
    "        \n",
    "        if test.ndim > 1:\n",
    "            m, d_ = test.shape\n",
    "        else:\n",
    "            m = len(test)\n",
    "            d_ = 1\n",
    "        \n",
    "        size = len(np.unique(y))\n",
    "        class_counts = np.zeros((m, size))\n",
    "        for tree in model:\n",
    "            # get out of bag indicies\n",
    "            if in_task:\n",
    "                prob_indices = _generate_unsampled_indices(tree.random_state, n)\n",
    "                # in_bag_idx = _generate_sample_indices(tree.random_state, n) # this is not behaving as i expected\n",
    "            else:\n",
    "                prob_indices = np.random.choice(range(n), size=int(subsample*n), replace=False)\n",
    "\n",
    "            leaf_nodes = self._get_leaves(tree)\n",
    "            unique_leaf_nodes = np.unique(leaf_nodes)\n",
    "\n",
    "            # get all node counts\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            # get probs for eval samples\n",
    "            posterior_class_counts = np.zeros((len(unique_leaf_nodes), size))\n",
    "\n",
    "            for prob_index in prob_indices:\n",
    "                temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "                posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "\n",
    "            # total number of points in a node\n",
    "            row_sums = posterior_class_counts.sum(axis=1)\n",
    "\n",
    "            # no divide by zero\n",
    "            row_sums[row_sums == 0] = 1\n",
    "\n",
    "            # posteriors\n",
    "            class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "            # posteriors with finite sampling correction\n",
    "\n",
    "            class_probs = self._finite_sample_correction(class_probs, row_sums)\n",
    "\n",
    "            # posteriors as a list\n",
    "            class_probs.tolist()\n",
    "\n",
    "            partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "            # get probability for out of bag samples\n",
    "            eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "            eval_class_probs = np.array(eval_class_probs)\n",
    "            # find total elements for out of bag samples\n",
    "            elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "            # store counts for each x (repeat fhis for each tree)\n",
    "            class_counts += elems\n",
    "        # calculate p(y|X = x) for all x's\n",
    "        probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def predict(self, test, representation=0, decider='all', subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each sample in test.\n",
    "        \n",
    "        Input\n",
    "        test: array-like; either a 1d array of length n_features\n",
    "            or a 2d array of shape (m, n_features) \n",
    "        task_number: int; task number \n",
    "        \"\"\"\n",
    "        \n",
    "        size=len(np.unique(self.y_[decider]))\n",
    "        sum_posteriors = np.zeros((test.shape[0], size))\n",
    "        \n",
    "        if representation is 'all':\n",
    "            for i in range(self.n_tasks):\n",
    "                sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                            i,\n",
    "                                                            decider,\n",
    "                                                            subsample,\n",
    "                                                            acorn)\n",
    "            \n",
    "        else:\n",
    "            sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                        representation,\n",
    "                                                        decider,\n",
    "                                                        subsample,\n",
    "                                                        acorn)\n",
    "                \n",
    "        return np.argmax(sum_posteriors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_rotation(theta=0, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    R = np.array([\n",
    "        [np.cos(theta), np.sin(theta)],\n",
    "        [-np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def generate_gaussian_parity(n, mean=np.array([-1, -1]), cov_scale=1, angle_params=None, k=1, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    d = len(mean)\n",
    "    \n",
    "    if mean[0] == -1 and mean[1] == -1:\n",
    "        mean = mean + 1 / 2**k\n",
    "    \n",
    "    mnt = np.random.multinomial(n, 1/(4**k) * np.ones(4**k))\n",
    "    cumsum = np.cumsum(mnt)\n",
    "    cumsum = np.concatenate(([0], cumsum))\n",
    "    \n",
    "    Y = np.zeros(n)\n",
    "    X = np.zeros((n, d))\n",
    "    \n",
    "    for i in range(2**k):\n",
    "        for j in range(2**k):\n",
    "            temp = np.random.multivariate_normal(mean, cov_scale * np.eye(d), \n",
    "                                                 size=mnt[i*(2**k) + j])\n",
    "            temp[:, 0] += i*(1/2**(k-1))\n",
    "            temp[:, 1] += j*(1/2**(k-1))\n",
    "            \n",
    "            X[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = temp\n",
    "            \n",
    "            if i % 2 == j % 2:\n",
    "                Y[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = 0\n",
    "            else:\n",
    "                Y[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = 1\n",
    "                \n",
    "    if d == 2:\n",
    "        if angle_params is None:\n",
    "            angle_params = np.random.uniform(0, 2*np.pi)\n",
    "            \n",
    "        R = generate_2d_rotation(angle_params)\n",
    "        X = X @ R\n",
    "    else:\n",
    "        raise ValueError('d=%i not implemented!'%(d))\n",
    "       \n",
    "    return X, Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifelong_experiment(nR, nx, funcx, paramsx, nz, funcz, paramsz, m=500, subsample=0.32, return_posteriors=False, mesh_='unit square', acorn=None):\n",
    "    if acorn is None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    errors = np.zeros((nR, 4))\n",
    "    \n",
    "    for i in range(nR):\n",
    "    \n",
    "        Tx = int(np.floor(np.sqrt(nx)))\n",
    "        Tz = int(np.floor(np.sqrt(nz)))\n",
    "\n",
    "        # Source task\n",
    "        X, labelsX = funcx(nx, *paramsx)\n",
    "        testX, test_labelsX = funcx(m, *paramsx)\n",
    "\n",
    "        # Target task\n",
    "        Z, labelsZ = funcz(nz, *paramsz)\n",
    "        testZ, test_labelsZ = funcz(m, *paramsz)\n",
    "\n",
    "\n",
    "        lifelong_forest = LifelongForest()\n",
    "\n",
    "        if nx == 0 and nz == 0:\n",
    "            raise ValueError('no samples')\n",
    "\n",
    "        elif nx == 0:\n",
    "            lifelong_forest.new_forest(Z, labelsZ, n_estimators=Tz)\n",
    "\n",
    "            df_task2=lifelong_forest.predict(testZ, representation=1, decider=1)\n",
    "            llf_task2=lifelong_forest.predict(testZ, representation='all', decider=1)\n",
    "\n",
    "            errors[i,0] = 0.5\n",
    "            errors[i,1] = 0.5  \n",
    "\n",
    "            errors[i,2] = 1 - np.sum(df_task2 == test_labelsZ)/m\n",
    "            errors[i,3] = 1 - np.sum(llf_task2 == test_labelsZ)/m\n",
    "\n",
    "        if nz == 0:\n",
    "            lifelong_forest.new_forest(X, labelsX, n_estimators=Tx)\n",
    "\n",
    "            df_task1=lifelong_forest.predict(testX, representation=0, decider=0)\n",
    "            llf_task1=lifelong_forest.predict(testX, representation='all', decider=0)\n",
    "\n",
    "            errors[i,0] = 1 - np.sum(df_task1 == test_labelsX)/m\n",
    "            errors[i,1] = 1 - np.sum(llf_task1 == test_labelsX)/m\n",
    "\n",
    "            errors[i,2] = 0.5\n",
    "            errors[i,3] = 0.5\n",
    "        else:\n",
    "            lifelong_forest.new_forest(X, labelsX, n_estimators=Tx)\n",
    "            lifelong_forest.new_forest(Z, labelsZ, n_estimators=Tz)\n",
    "\n",
    "            df_task1=lifelong_forest.predict(testX, representation=0, decider=0)\n",
    "            llf_task1=lifelong_forest.predict(testX, representation='all', decider=0)\n",
    "\n",
    "            df_task2=lifelong_forest.predict(testZ, representation=1, decider=1)\n",
    "            llf_task2=lifelong_forest.predict(testZ, representation='all', decider=1)\n",
    "\n",
    "            errors[i,0] = 1 - np.sum(df_task1 == test_labelsX)/m\n",
    "            errors[i,1] = 1 - np.sum(llf_task1 == test_labelsX)/m \n",
    "\n",
    "            errors[i,2] = 1 - np.sum(df_task2 == test_labelsZ)/m\n",
    "            errors[i,3] = 1 - np.sum(llf_task2 == test_labelsZ)/m\n",
    "    \n",
    "    if return_posteriors:\n",
    "        import itertools\n",
    "        # returns posteriors for a fixed mesh..\n",
    "        if mesh_ == 'unit square':\n",
    "            mesh_=np.array(list(itertools.product(np.arange(-1, 1+0.05, step=0.05), np.arange(-1, 1+0.05, step=0.05))))\n",
    "        \n",
    "        posteriors_structX_estX=lifelong_forest._estimate_posteriors(mesh_, representation=0, decider=0)\n",
    "        posteriors_structZ_estX=lifelong_forest._estimate_posteriors(mesh_, representation=1, decider=0)\n",
    "        \n",
    "        posteriors_structZ_estZ=lifelong_forest._estimate_posteriors(mesh_, representation=1, decider=1)\n",
    "        posteriors_structX_estZ=lifelong_forest._estimate_posteriors(mesh_, representation=0, decider=1)\n",
    "                    \n",
    "        return posteriors_structX_estX, posteriors_structZ_estX, posteriors_structZ_estZ, posteriors_structX_estZ\n",
    "    else:\n",
    "        return np.mean(errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1228daf906d4ac1b2694b9165cf658e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_mc = 50 # number of simulation repititions\n",
    "\n",
    "n_xor = 100\n",
    "n_rxor = 100 # number of training samples \n",
    "\n",
    "thetas = np.arange(0, 180)\n",
    "\n",
    "m = 1000 # number of test samples each monte carlo iteration\n",
    "\n",
    "mean_error_xor_rxor = np.zeros((4, len(thetas)))\n",
    "std_error_xor_rxor = np.zeros((4, len(thetas)))\n",
    "\n",
    "mean_te_xor_rxor = np.zeros((2, len(thetas)))\n",
    "std_te_xor_rxor = np.zeros((2, len(thetas)))\n",
    "\n",
    "nR=30\n",
    "\n",
    "task1_func = generate_gaussian_parity\n",
    "task1_params = (np.array([-1, -1]), 0.1, 0, 1)\n",
    "\n",
    "task2_func = generate_gaussian_parity\n",
    "\n",
    "d = 2\n",
    "for i, t in enumerate(tqdm(thetas)):\n",
    "    task2_params= (np.array([-1, -1]), 0.1, t * (np.pi / 180), 1)\n",
    "    condensed_func = lambda x : lifelong_experiment(x, n_xor, \n",
    "                                                    task1_func, task1_params, \n",
    "                                                    n_rxor, \n",
    "                                                    task2_func, task2_params, \n",
    "                                                    m=m)\n",
    "    temp_errors = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in nR*np.ones(n_mc)))\n",
    "    \n",
    "    mean_error_xor_rxor[:, i] = np.mean(temp_errors, axis=0)\n",
    "    std_error_xor_rxor[:, i] = np.std(temp_errors, ddof=1, axis=0) \n",
    "        \n",
    "    mean_te_xor_rxor[0, i] = np.mean(temp_errors[:, 0] / temp_errors[:, 1])\n",
    "    mean_te_xor_rxor[1, i] = np.mean(temp_errors[:, 2] / temp_errors[:, 3])\n",
    "    \n",
    "    std_te_xor_rxor[0, i] = np.std(temp_errors[:, 0] / temp_errors[:, 1], axis=0, ddof=1)\n",
    "    std_te_xor_rxor[1, i] = np.std(temp_errors[:, 2] / temp_errors[:, 3], axis=0, ddof=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(mean_error_xor_rxor, open('data/gaussian-rxor-sweep/mean_error_rxor_sweep_nmc%i_nD%i_nxor%i_nrxor%i_m%i.pkl'%(n_mc, nR,\n",
    "                                                                                                                         n_xor, n_rxor,\n",
    "                                                                                                                         m),\n",
    "                                      'wb'))\n",
    "                                    \n",
    "\n",
    "pickle.dump(std_error_xor_rxor, open('data/gaussian-rxor-sweep/std_error_rxor_sweep_nmc%i_nD%i_nxor%i_nrxor%i_m%i.pkl'%(n_mc, nR,\n",
    "                                                                                                                         n_xor, n_rxor,\n",
    "                                                                                                                         m),\n",
    "                                      'wb'))\n",
    "\n",
    "pickle.dump(mean_te_xor_rxor, open('data/gaussian-rxor-sweep/mean_te_rxor_sweep_nmc%i_nD%i_nxor%i_nrxor%i_m%i.pkl'%(n_mc, nR,\n",
    "                                                                                                                         n_xor, n_rxor,\n",
    "                                                                                                                         m),\n",
    "                                      'wb'))\n",
    "\n",
    "pickle.dump(std_te_xor_rxor, open('data/gaussian-rxor-sweep/std_te_rxor_sweep_nmc%i_nD%i_nxor%i_nrxor%i_m%i.pkl'%(n_mc, nR,\n",
    "                                                                                                                         n_xor, n_rxor,\n",
    "                                                                                                                         m),\n",
    "                                      'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
